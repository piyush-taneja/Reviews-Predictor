{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                               Natural Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author: piyush taneja\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv' , delimiter = '\\t' , quoting = 3)\n",
    "# Here we used delimiter as parameters are seperated by tab space and quoting is used to prevent errors created by \"\" so its value is 3 to ignore \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\piyush\n",
      "[nltk_data]     taneja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import  re\n",
    "import nltk\n",
    "nltk.download('stopwords')  # This is used to download the set ofwords which are irrelevant in review like this , is ,an, etc \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = [] \n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]' , ' ' , dataset['Review'][i])\n",
    "    review = review.lower()  \n",
    "    #We used sub() function which removes all nos. and punchuation but keep only the letters\n",
    "    # We used ^a-zA-Z to make that it does not remove any letter a-z or A-Z\n",
    "    # We ised ' ' to add space b/w the replaced thing/character and thing which is before that thing/character\n",
    "    # lower() function is used to convert all the letters to lower case\n",
    "    review = review.split() #It is used to split the whole sentence into a set of words\n",
    "    ps = PorterStemmer()   #Stem function is used to conver into same words like converting laved to love\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  # set is used so to make the list into sets as it is implemented faster.\n",
    "    review = ' '.join(review)  # Here we used join to convert list back into string and used ' '  to give space b/w words\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500) #Here we used max_features = 1500 as it keep only 1500 frequent features and removes rest of unfrequent features. Here we are dealing with sparcity\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling is not required as we only have 0 and 1 so we removed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting classifier to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix  #Here confusion_matrix is a function and not a class as class has capitalletters at the beginning.\n",
    "cm = confusion_matrix(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the confusion Matrix\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "(55+91)/(55+91+12+42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208955223880597"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the Precision\n",
    "(55)/(55+12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670103092783505"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the Recall\n",
    "(55)/(55+42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
